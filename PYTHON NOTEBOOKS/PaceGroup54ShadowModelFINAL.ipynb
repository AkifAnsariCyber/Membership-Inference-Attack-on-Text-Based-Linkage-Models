{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4763c18-0e9f-4601-a3e8-309764173be0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "GROUP 54 SHADOW MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d4c350c-d96c-4be3-96ca-be141c2d2cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.12/site-packages (3.5.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: seaborn in /opt/anaconda3/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /opt/anaconda3/lib/python3.12/site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/anaconda3/lib/python3.12/site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/anaconda3/lib/python3.12/site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Requirement already satisfied: tf_keras in /opt/anaconda3/lib/python3.12/site-packages (2.19.0)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in /opt/anaconda3/lib/python3.12/site-packages (from tf_keras) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (3.9.2)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (0.5.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf_keras) (0.44.0)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (13.7.1)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (0.0.9)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (0.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf_keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf_keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf_keras) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf_keras) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf_keras) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf_keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf_keras) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf_keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "# =========\n",
    "# installs\n",
    "# =========\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install scikit-learn\n",
    "!pip install seaborn\n",
    "!pip install tf_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82e5005a-c8a0-4dfb-8aea-4e632509e83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========\n",
    "# Imports\n",
    "# ========\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.models    import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers    import Input, Dense, LeakyReLU, Concatenate\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.metrics            import accuracy_score, f1_score, classification_report\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3766284-7826-4f2d-8d74-42a6843ec1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1_shadow_train shape: (32000, 768)\n",
      "x2_shadow_train shape: (32000, 768)\n",
      "y_shadow_train shape: (32000,)\n",
      "x1_shadow_test shape: (8000, 768)\n",
      "x2_shadow_test shape: (8000, 768)\n",
      "y_shadow_test shape: (8000,)\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Step 1: Loading Embeddings\n",
    "# ===========================\n",
    "x1_shadow_train = np.load(\"x1_shadow_train.npy\")\n",
    "x2_shadow_train = np.load(\"x2_shadow_train.npy\")\n",
    "y_shadow_train  = np.load(\"y_shadow_train.npy\")\n",
    "x1_shadow_test  = np.load(\"x1_shadow_test.npy\")\n",
    "x2_shadow_test  = np.load(\"x2_shadow_test.npy\")\n",
    "y_shadow_test   = np.load(\"y_shadow_test.npy\")\n",
    "\n",
    "print(\"x1_shadow_train shape:\", x1_shadow_train.shape)\n",
    "print(\"x2_shadow_train shape:\", x2_shadow_train.shape)\n",
    "print(\"y_shadow_train shape:\", y_shadow_train.shape)\n",
    "print(\"x1_shadow_test shape:\", x1_shadow_test.shape)\n",
    "print(\"x2_shadow_test shape:\", x2_shadow_test.shape)\n",
    "print(\"y_shadow_test shape:\", y_shadow_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5281417a-ef2d-4689-ad31-a4fd78e787ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# Step 3: Build Siamese Autoencoder\n",
    "# ==================================\n",
    "def build_siamese_autoencoder(embedding_dim):\n",
    "    encoder_input = Input(shape=(embedding_dim,))\n",
    "    x = layers.Dense(50, activity_regularizer=regularizers.l1(0.01))(encoder_input)\n",
    "    x = layers.LeakyReLU(alpha=0.01)(x)\n",
    "    encoder_output = layers.Dense(embedding_dim, activation='relu')(x)\n",
    "    encoder = Model(encoder_input, encoder_output)\n",
    "\n",
    "    decoder_input = Input(shape=(embedding_dim,))\n",
    "    decoder_output = layers.Dense(embedding_dim, activation='sigmoid')(decoder_input)\n",
    "    decoder = Model(decoder_input, decoder_output)\n",
    "\n",
    "    input1 = Input(shape=(embedding_dim,))\n",
    "    input2 = Input(shape=(embedding_dim,))\n",
    "    encoded1 = encoder(input1)\n",
    "    encoded2 = encoder(input2)\n",
    "    recon1 = decoder(encoded1)\n",
    "    recon2 = decoder(encoded2)\n",
    "\n",
    "    merged_output = layers.Concatenate()([recon1, recon2])\n",
    "    model = Model(inputs=[input1, input2], outputs=merged_output)\n",
    "    return model, encoder\n",
    "\n",
    "def hybrid_classification_loss(margin=2.5, alpha=1.0):\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        emb_dim = tf.shape(y_pred)[1] // 2\n",
    "        recon1 = y_pred[:, :emb_dim]\n",
    "        recon2 = y_pred[:, emb_dim:]\n",
    "        recon_loss = tf.reduce_mean(tf.square(recon1 - recon2), axis=1)\n",
    "        distances = tf.sqrt(tf.reduce_sum(tf.square(recon1 - recon2), axis=1))\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        contrastive_loss = y_true * tf.square(distances) + (1 - y_true) * tf.square(tf.maximum(margin - distances, 0))\n",
    "        return tf.reduce_mean(alpha * recon_loss + contrastive_loss)\n",
    "    return loss_fn\n",
    "embedding_dim = x1_shadow_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1ea7287-ad49-4746-84e5-330cef967dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Step 4: Split Train & Test Embeddings into Number of Shadow Models\n",
    "# ===================================================================\n",
    "numShadowModels = 3;\n",
    "\n",
    "n_train = len(y_shadow_train)\n",
    "idx_splits_train = np.array_split(np.arange(n_train), numShadowModels)\n",
    "\n",
    "n_test = len(y_shadow_test)\n",
    "idx_splits_test = np.array_split(np.arange(n_test), numShadowModels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff19abe6-73e8-4722-8c16-17370484cf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training shadow model #1 on 10667 examples ---\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 66.9716 - val_loss: 24.6692\n",
      "Epoch 2/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 23.3380 - val_loss: 18.6405\n",
      "Epoch 3/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 18.4217 - val_loss: 16.2679\n",
      "Epoch 4/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 16.0546 - val_loss: 14.8947\n",
      "Epoch 5/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 14.5832 - val_loss: 13.6938\n",
      "Epoch 6/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 13.5336 - val_loss: 13.0284\n",
      "Epoch 7/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 12.7601 - val_loss: 12.2633\n",
      "Epoch 8/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 12.0031 - val_loss: 11.7428\n",
      "Epoch 9/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 11.4527 - val_loss: 11.4217\n",
      "Epoch 10/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 10.9567 - val_loss: 10.9169\n",
      "Epoch 11/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 10.4930 - val_loss: 10.5941\n",
      "Epoch 12/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 10.0940 - val_loss: 10.1882\n",
      "Epoch 13/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 9.8686 - val_loss: 9.9751\n",
      "Epoch 14/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 9.5595 - val_loss: 9.5828\n",
      "Epoch 15/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 9.2946 - val_loss: 9.2398\n",
      "Epoch 16/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 9.0837 - val_loss: 9.1677\n",
      "Epoch 17/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.8222 - val_loss: 9.2746\n",
      "Epoch 18/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.6970 - val_loss: 8.8588\n",
      "Epoch 19/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.5039 - val_loss: 8.3143\n",
      "Epoch 20/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.1402 - val_loss: 8.7198\n",
      "Epoch 21/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.2162 - val_loss: 8.0193\n",
      "Epoch 22/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 7.9083 - val_loss: 9.0010\n",
      "Epoch 23/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.1218 - val_loss: 7.9162\n",
      "Epoch 24/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.7231 - val_loss: 8.2322\n",
      "Epoch 25/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.6737 - val_loss: 8.8939\n",
      "Epoch 26/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.9261 - val_loss: 7.7069\n",
      "Epoch 27/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.3948 - val_loss: 7.7241\n",
      "Epoch 28/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 7.3680 - val_loss: 7.4036\n",
      "Epoch 29/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.1590 - val_loss: 7.5971\n",
      "Epoch 30/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.2954 - val_loss: 7.3583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training shadow model #2 on 10667 examples ---\n",
      "Epoch 1/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 61.0257 - val_loss: 23.8255\n",
      "Epoch 2/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 22.3845 - val_loss: 18.1925\n",
      "Epoch 3/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 17.8508 - val_loss: 15.8599\n",
      "Epoch 4/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 15.6411 - val_loss: 14.4205\n",
      "Epoch 5/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 14.2361 - val_loss: 13.3220\n",
      "Epoch 6/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 13.1565 - val_loss: 12.4605\n",
      "Epoch 7/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 12.3965 - val_loss: 11.8270\n",
      "Epoch 8/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 11.6403 - val_loss: 11.4216\n",
      "Epoch 9/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 11.2156 - val_loss: 10.9482\n",
      "Epoch 10/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 10.7717 - val_loss: 10.7801\n",
      "Epoch 11/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 10.4003 - val_loss: 10.2372\n",
      "Epoch 12/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 9.9131 - val_loss: 9.8556\n",
      "Epoch 13/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 9.5638 - val_loss: 9.9449\n",
      "Epoch 14/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 9.4540 - val_loss: 9.4887\n",
      "Epoch 15/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 9.1723 - val_loss: 9.3576\n",
      "Epoch 16/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 8.9840 - val_loss: 9.4053\n",
      "Epoch 17/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.9456 - val_loss: 9.2620\n",
      "Epoch 18/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.7491 - val_loss: 8.5361\n",
      "Epoch 19/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.2711 - val_loss: 8.6221\n",
      "Epoch 20/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.2433 - val_loss: 8.9040\n",
      "Epoch 21/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 8.2293 - val_loss: 8.6423\n",
      "Epoch 22/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.1075 - val_loss: 8.5939\n",
      "Epoch 23/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.9989 - val_loss: 8.0435\n",
      "Epoch 24/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.5090 - val_loss: 8.1241\n",
      "Epoch 25/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.6625 - val_loss: 8.2996\n",
      "Epoch 26/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.5925 - val_loss: 8.0812\n",
      "Epoch 27/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.4658 - val_loss: 7.7332\n",
      "Epoch 28/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.2612 - val_loss: 7.8427\n",
      "Epoch 29/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.3077 - val_loss: 7.5519\n",
      "Epoch 30/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.0686 - val_loss: 7.7699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training shadow model #3 on 10666 examples ---\n",
      "Epoch 1/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 67.7522 - val_loss: 24.4581\n",
      "Epoch 2/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 22.8117 - val_loss: 18.5328\n",
      "Epoch 3/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 18.1141 - val_loss: 16.0258\n",
      "Epoch 4/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 15.9715 - val_loss: 14.7507\n",
      "Epoch 5/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 14.4819 - val_loss: 13.5110\n",
      "Epoch 6/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 13.3473 - val_loss: 12.7058\n",
      "Epoch 7/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 12.5481 - val_loss: 12.0264\n",
      "Epoch 8/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 11.9352 - val_loss: 11.5286\n",
      "Epoch 9/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 11.2536 - val_loss: 11.2093\n",
      "Epoch 10/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 10.8482 - val_loss: 10.6923\n",
      "Epoch 11/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 10.4409 - val_loss: 10.8062\n",
      "Epoch 12/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 10.2280 - val_loss: 10.1949\n",
      "Epoch 13/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 9.8012 - val_loss: 9.7641\n",
      "Epoch 14/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 9.4110 - val_loss: 9.7446\n",
      "Epoch 15/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 9.2400 - val_loss: 9.5791\n",
      "Epoch 16/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 9.0532 - val_loss: 8.7841\n",
      "Epoch 17/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.6227 - val_loss: 8.8420\n",
      "Epoch 18/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.4623 - val_loss: 8.6913\n",
      "Epoch 19/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.3067 - val_loss: 9.0537\n",
      "Epoch 20/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.4784 - val_loss: 8.8925\n",
      "Epoch 21/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.3669 - val_loss: 8.4525\n",
      "Epoch 22/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.9679 - val_loss: 8.1478\n",
      "Epoch 23/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.8596 - val_loss: 8.4367\n",
      "Epoch 24/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.8933 - val_loss: 8.5873\n",
      "Epoch 25/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 7.6912 - val_loss: 8.1345\n",
      "Epoch 26/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.5223 - val_loss: 7.8749\n",
      "Epoch 27/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.3713 - val_loss: 7.9336\n",
      "Epoch 28/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.3015 - val_loss: 7.6596\n",
      "Epoch 29/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.2167 - val_loss: 7.4877\n",
      "Epoch 30/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.1217 - val_loss: 7.2526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# Step 5: Train Three Independent Shadow Models\n",
    "# ==============================================\n",
    "shadow_models = []\n",
    "shadow_mlps = []\n",
    "shadow_encoders = []\n",
    "\n",
    "for i in range(numShadowModels):\n",
    "    train_idxs = idx_splits_train[i]\n",
    "    x1_i = x1_shadow_train[train_idxs]\n",
    "    x2_i = x2_shadow_train[train_idxs]\n",
    "    y_i  = y_shadow_train[train_idxs]\n",
    "\n",
    "    model_i, encoder_i = build_siamese_autoencoder(embedding_dim)\n",
    "    model_i.compile(\n",
    "        optimizer='adam',\n",
    "        loss=hybrid_classification_loss(margin=2.5, alpha=1.0)\n",
    "    )\n",
    "\n",
    "    print(f\"\\n--- Training shadow model #{i+1} on {len(train_idxs)} examples ---\")\n",
    "    model_i.fit(\n",
    "        [x1_i, x2_i], y_i,\n",
    "        epochs=30, batch_size=256,\n",
    "        validation_split=0.1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    shadow_models.append(model_i)\n",
    "    shadow_encoders.append(encoder_i)\n",
    "\n",
    "    encoder_i.save(f\"shadow_encoder_{i+1}.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b24d2c75-ab17-4191-92e1-9cf71619658c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating Shadow Model #1 ===\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 415us/step\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 283us/step\n",
      "Distance‐based → Acc: 0.5028, F1: 0.6683\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231us/step\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217us/step\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321us/step\n",
      "Classifier @0.5 → Acc: 0.7904, F1: 0.7870\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.81      0.79      1331\n",
      "         1.0       0.80      0.77      0.79      1336\n",
      "\n",
      "    accuracy                           0.79      2667\n",
      "   macro avg       0.79      0.79      0.79      2667\n",
      "weighted avg       0.79      0.79      0.79      2667\n",
      "\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365us/step\n",
      "Tuned threshold on pseudo‐val → 0.51 (F1=0.8518)\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final eval @thr=0.51 → Acc: 0.7968, F1: 0.8012\n",
      "\n",
      "=== Evaluating Shadow Model #2 ===\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350us/step\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271us/step\n",
      "Distance‐based → Acc: 0.4949, F1: 0.6622\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221us/step\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223us/step\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326us/step\n",
      "Classifier @0.5 → Acc: 0.7777, F1: 0.7635\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.83      0.79      1347\n",
      "         1.0       0.81      0.72      0.76      1320\n",
      "\n",
      "    accuracy                           0.78      2667\n",
      "   macro avg       0.78      0.78      0.78      2667\n",
      "weighted avg       0.78      0.78      0.78      2667\n",
      "\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step\n",
      "Tuned threshold on pseudo‐val → 0.50 (F1=0.8552)\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final eval @thr=0.50 → Acc: 0.7683, F1: 0.7758\n",
      "\n",
      "=== Evaluating Shadow Model #3 ===\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339us/step\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264us/step\n",
      "Distance‐based → Acc: 0.5098, F1: 0.6741\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219us/step\n",
      "\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218us/step\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322us/step\n",
      "Classifier @0.5 → Acc: 0.7787, F1: 0.7823\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.77      0.77      1314\n",
      "         1.0       0.78      0.78      0.78      1352\n",
      "\n",
      "    accuracy                           0.78      2666\n",
      "   macro avg       0.78      0.78      0.78      2666\n",
      "weighted avg       0.78      0.78      0.78      2666\n",
      "\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363us/step\n",
      "Tuned threshold on pseudo‐val → 0.44 (F1=0.8652)\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final eval @thr=0.44 → Acc: 0.7761, F1: 0.7918\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Step 6: Evaluate & Add Classification Heads for Each Shadow\n",
    "# ============================================================\n",
    "for i in range(numShadowModels):\n",
    "    print(f\"\\n=== Evaluating Shadow Model #{i+1} ===\")\n",
    "\n",
    "    # grab this shadow’s encoder\n",
    "    encoder_i = shadow_encoders[i]\n",
    "\n",
    "    # select this model's test indices\n",
    "    test_idxs = idx_splits_test[i]\n",
    "    x1_test_i = x1_shadow_test[test_idxs]\n",
    "    x2_test_i = x2_shadow_test[test_idxs]\n",
    "    y_test_i  = y_shadow_test[test_idxs]\n",
    "\n",
    "    # 1) Distance‐based evaluation on split test\n",
    "    encoded1_test = encoder_i.predict(x1_test_i)\n",
    "    encoded2_test = encoder_i.predict(x2_test_i)\n",
    "    test_distances = np.linalg.norm(encoded1_test - encoded2_test, axis=1)\n",
    "\n",
    "    threshold = 1.0\n",
    "    test_pred = (test_distances < threshold).astype(int)\n",
    "\n",
    "    acc_dist = accuracy_score(y_test_i, test_pred)\n",
    "    f1_dist  = f1_score(y_test_i, test_pred)\n",
    "    print(f\"Distance‐based → Acc: {acc_dist:.4f}, F1: {f1_dist:.4f}\")\n",
    "\n",
    "    # 2) Siamese‐style MLP classification on this split\n",
    "    train_idxs = idx_splits_train[i]\n",
    "    encoded1_tr = encoder_i.predict(x1_shadow_train[train_idxs])\n",
    "    encoded2_tr = encoder_i.predict(x2_shadow_train[train_idxs])\n",
    "    diff_train  = np.abs(encoded1_tr - encoded2_tr)\n",
    "    diff_test   = np.abs(encoded1_test - encoded2_test)\n",
    "\n",
    "    input_diff = Input(shape=(diff_train.shape[1],))\n",
    "    x = Dense(64, activation='relu')(input_diff)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "    clf_model = Model(inputs=input_diff, outputs=output)\n",
    "    clf_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    clf_model.fit(\n",
    "        diff_train, y_shadow_train[train_idxs],\n",
    "        epochs=20, batch_size=256,\n",
    "        validation_split=0.1,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    y_prob = clf_model.predict(diff_test).flatten()\n",
    "    y_pred = (y_prob > 0.5).astype(int)\n",
    "\n",
    "    acc_clf = accuracy_score(y_test_i, y_pred)\n",
    "    f1_clf  = f1_score(y_test_i, y_pred)\n",
    "    print(f\"Classifier @0.5 → Acc: {acc_clf:.4f}, F1: {f1_clf:.4f}\")\n",
    "    print(classification_report(y_test_i, y_pred))\n",
    "\n",
    "    # 3) Tune threshold on pseudo‐val split\n",
    "    diff_full = diff_train\n",
    "    y_full    = y_shadow_train[train_idxs]\n",
    "    d_tr, d_val, y_tr, y_val = train_test_split(\n",
    "        diff_full, y_full,\n",
    "        test_size=0.2, random_state=42,\n",
    "        stratify=y_full\n",
    "    )\n",
    "\n",
    "    input_d2 = Input(shape=(d_tr.shape[1],))\n",
    "    y2 = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01))(input_d2)\n",
    "    y2 = Dropout(0.5)(y2)\n",
    "    y2 = Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01))(y2)\n",
    "    out2 = Dense(1, activation='sigmoid')(y2)\n",
    "    clf2 = Model(inputs=input_d2, outputs=out2)\n",
    "    clf2.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    clf2.fit(d_tr, y_tr, epochs=30, batch_size=256, validation_split=0.1, verbose=0)\n",
    "\n",
    "    y_val_prob = clf2.predict(d_val).flatten()\n",
    "    best_thr, best_f1 = 0.5, 0.0\n",
    "    for thr in np.arange(0.1, 0.9, 0.01):\n",
    "        yv = (y_val_prob >= thr).astype(int)\n",
    "        _, _, f1s, _ = precision_recall_fscore_support(y_val, yv, average='binary')\n",
    "        if f1s > best_f1:\n",
    "            best_f1, best_thr = f1s, thr\n",
    "\n",
    "    print(f\"Tuned threshold on pseudo‐val → {best_thr:.2f} (F1={best_f1:.4f})\")\n",
    "\n",
    "    y_test_prob = clf2.predict(diff_test).flatten()\n",
    "    y_test_pred = (y_test_prob >= best_thr).astype(int)\n",
    "    acc_final = accuracy_score(y_test_i, y_test_pred)\n",
    "    f1_final  = f1_score(y_test_i, y_test_pred)\n",
    "    print(f\"Final eval @thr={best_thr:.2f} → Acc: {acc_final:.4f}, F1: {f1_final:.4f}\")\n",
    "\n",
    "    clf2.save(f\"shadow_mlp_{i+1}.h5\")\n",
    "    np.save(f\"shadow_threshold_{i+1}.npy\", np.array([best_thr]))\n",
    "\n",
    "    # Save predictions for this shadow's test split\n",
    "    pd.DataFrame({\n",
    "        \"True Label\": y_test_i,\n",
    "        \"Predicted Label\": y_test_pred,\n",
    "        \"Confidence\": y_test_prob\n",
    "    }).to_csv(f\"shadow{i+1}_test_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a72a89-6d10-4c3f-904d-aa3f97c05521",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
